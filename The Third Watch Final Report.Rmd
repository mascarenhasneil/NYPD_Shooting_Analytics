---
title: "Team 2: The Third Watch <br> <br> NYPD Shootout Analysis Report <br><br>" 
author: "<font size =6, color='white'>Dr Vladimir Shapiro</font>"
date: "<font size =6, color='white'>May 21, 2021</font>"

output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    math: mathjax
    number_sections: FALSE
    css: "images//style.css"


---

```{=html}
<p style="font-size:15px;text-align:left;"><i>Source: https://images.app.goo.gl/YStTSgPUfhG5JeYn7</i></p>
<center>
College of Professional Studies, Northeastern University<br>
ALY6015 80625 Intermediate Analytics SEC 04 <br>
Spring 2021 CPS [BOS-A-HY]<br>
<strong>Team Members: </strong>
Aswin Kumar Rajendran, 
Chaya Kotturesh 
and Neil Mascarenhas
<br><br><br><br><br>
</center>

```

```{r warning=FALSE, error=FALSE, message=FALSE, eval=TRUE, echo=FALSE, cache=TRUE, eval=TRUE}

'%ni%' <- Negate('%in%')

### We are creating this function to generate unique figure Ids in continuous order to comply with APA guidelines and use them while we are referring to them.  

Figure_Count <<- 0   ### Sets the count to initial 0  << means that the variable is global
GetFigureCount <- function()  ### Creating the function and it will be used when we need to add a figure id. 
{Figure_Count <<- Figure_Count + 1  ## Increments the count so that we get new id every single tile
return(paste("Figure :",Figure_Count))}### Returns back the values, later to be used in the assignment.


installAllPackages <- function(Pkgs){
    new.Pkgs <- Pkgs[!(Pkgs %in% installed.packages()[, "Package"])]
    if (length(new.Pkgs)) 
        install.packages(new.Pkgs, dependencies = TRUE)
    null<- sapply(Pkgs, require, character.only = TRUE)
    
}


options(scipen=5)   #  A penalty to be applied when deciding to print numeric values in fixed or exponential notation.
options(digits = 3)   # Globally sets 3 places after decimal point
options(knitr.kable.NA = '')
knitr::opts_chunk$set(tidy=TRUE)  ## Reformat the R code
knitr::opts_chunk$set(tidy.opts = list(blank = FALSE, width.cutoff = 100))  ## Removed blank lines and try to cut the code lines at set(100) characters.
knitr::opts_chunk$set(cache=TRUE)   ## Make execution faster. When evaluating code chunks for the second time, the cached chunks are skipped (unless they have been modified).
knitr::opts_chunk$set(comment = ' ')  ## remove the default ## from the output prefix.
knitr::opts_chunk$set(fig.align = "Center") ## All figures center align.

```
```{=html}
<hr><p id="MainPoint"> Introduction </p><hr>
```



This paper summarizes our preliminary (Exploratory Data Analysis - EDA) analysis on the selected dataset, the NYPD shooting incident dataset for this group project. It will share some insightful results from the EDA and descriptive statistics. Further, this paper identifies a set of methods used to answer the business questions on the dataset and justifies those methods using statistics and analytics concepts. 



#### First, let us create the environment by importing the required packages to analyze and create a visualization on the dataset.


```{r warning=FALSE, error=FALSE, message=FALSE, eval=TRUE, echo=FALSE, tidy=FALSE, tidy.opts = list(blank = FALSE, width.cutoff = 100), cache=FALSE}
packages <- c(
    "dplyr", "psych", "plyr", "reshape2", "rlang",  "patchwork",  ## Tools to work with core language features of R and the tidyverse
    "scales", "grid", "UsingR", "tidyr", "tidyverse", ## For Data cleaning and processing
    "readxl", ## This package makes it possible to read Excel (.xlsx) files to r.
    "ggplot2","ggpubr", "RColorBrewer", "gplots",
    "rmarkdown","formatR",  ## To format R markdown and add features
    "plotly","gmodels",   ## To plot realistic, interactive plots
    "knitr","kableExtra", ## To display tables in beautiful format.
    "base64","base64enc", ## To use Base 64 encryption 
    "enc",        ## DOC: https://cran.r-project.org/web/packages/enc/enc.pdf
    "EDA",        ## DOC: https://cran.r-project.org/web/packages/dlookr/vignettes/EDA.html
    "hrbrthemes", ## Adds extra themes for html rendering
    "ggcorrplot",  ## Extensively for Correlation Plots in ggplot",
    "corrplot",    ## For visualizing correlation matrices and confidence intervals. It also contains some algorithms to do matrix reordering
    "Publish", "magrittr", ### Used to add column names with piping
    "PerformanceAnalytics", ### Used to Draw scatter plots on Correlation matrix
    "forecast", ## Required for the EDA Report
    "xfun",   ## Is used to add eternal files and read them to this Markdown. 
    "htmltools", ## Helps to display HTML in the Markdown (and pdf when we knit)
    "funModeling", # Used to run Status function.
    "corrgram", # Used to Plot corplots using ggplot engine.
    "dummies", # Used to create dummies
    "lsmeans", ## Used to plot lm
    "imputeTS", ## Used by na.mean() function
    "leaps", ## Best for Subsets regsubsets()  
    "lmSubsets", ## For all subset function
    "sjPlot", "nhstplot", ## Used to plot Chi-Square
    "gtsummary", ## to have better summary of data
    "ROCR",    ## Used to check for performance 
    "InformationValue",  ## Used by optimalCutoff function
    "caTools", ## Used to Split data into test and train.
    "coefplot", ## Used to plot Coefficients.
    "mltools",  ## Calculate Root-Mean-Square Error (Deviation)
    "caret",      ## Used to generate confusion matrix
    "glmnet",   ## To run ridge and lasso regularization on models using cv.glmnet and glmnet function.
    "descr", "vcd", ## To run autocrads test.
    "mlbench", ### Machine learning baenchmarking 
    "randomForest", ## To model randomForests and run plots
    "epiDisplay"#,"psychTools", "moments", "Hmisc","skimr", "DataExplore", "visdat", "mapview" 
)
    
installAllPackages(packages)
# 
# if(installAllPackages(packages)) 
#     cat("Done, Installed and imported All the required packages and libraries for this assignment") else 
#     cat("Some Error in installation")
#     cat("Done")
#     
```


## Business Questions
1.	Is there a relationship between the Race of the victim and the Perpetrator who died in the shooting incident? *by Chaya Kotturesh*

2.	Is there any noticeable relationship between the Perpetrator's Race to the victim's sex and age with a motive to recognize the Perpetrator's race pattern? *by Neil Mascarenhas*

3.	Recognize the pattern/relationship between the victim's age and the Location, specifically Bar/Night Club, to recognize the trend throughout the years and increase the patrolling.  *by Aswin Kumar Rajendran*


```{=html}
<hr><p id="MainPoint"> Analysis and Interpretations </p><hr>
```


```{r}

NYPD = data.frame(read_csv("Data\\NYPD_Shooting_Incident_Data__Historic_.csv", skip_empty_rows = TRUE), stringsAsFactors = T)


### Removing unwanted variables/ columns
### 

NYPD <- NYPD %>%
    select(-INCIDENT_KEY, -X_COORD_CD, -Y_COORD_CD, -Latitude, -Longitude, -Lon_Lat)


NYPD[duplicated(NYPD$INCIDENT_KEY),]



Datatypelist<-as.data.frame(skimr::skim(NYPD)[[1]],skimr::skim(NYPD)[[2]])

knitr::kable(Datatypelist, "pipe", col.names = "Data Type")

#$`skimr::skim(NYPD)[[1]]`=="character"

```



```{r}
#char_vector <- c("OCCUR_DATE","BORO","LOCATION_DESC","PERP_AGE_GROUP","PERP_SEX","PERP_RACE","VIC_AGE_GROUP","VIC_SEX","VIC_RACE","Lon_Lat")

# Putting unknown for missing string values
NYPD[Datatypelist$`skimr::skim(NYPD)[[1]]`=="character"][is.na(NYPD[Datatypelist$`skimr::skim(NYPD)[[1]]`=="character"])] <- "UNKNOWN"


# Putting 0 for missing numeric values
NYPD[Datatypelist$`skimr::skim(NYPD)[[1]]`=="numeric"][is.na(NYPD[Datatypelist$`skimr::skim(NYPD)[[1]]`=="numeric"])] <- 0
```


#### We have taken care of missing values. Lets us begin with our analysis and understand the dataset. Here we go through each variable/feature column and get meaningful insights from them. As we proceed, we will keep updating our dataset so that by the end, we get a complete dataset with variables and values we need for our further analysis.


#### We can see above that the frequency of the Boro is plotted. For this dataset, we have five Boros. Among them, Brooklyn has the highest records, while Staten Island has the lowest record of shootouts. Can we say that Brooklyn is more dangerous than the rest of Boros? Let us find it out further in our report.





```{=html}
<hr><p id="MainPoint"> Methodology </p><hr>
```



### Here, we proceed with identifying the methods we will be using, along with justification for those methods.

# Business Questions

## 1.	Is there a relationship between the Race of the victim and the Perpetrator who died in the shooting incident? *Major contribution by Chaya Kotturesh*

#### Here, we will use the Generalized regression model to solve this question. We know that Black people have a higher count of shootings when compared with others. Here are predictable variable is the Perpetrator's race, and the independent variable will be Victim race. We will create a model for those who are dead in the shootings.



```{r warning=FALSE, error=FALSE}
nypd_df <- read.csv("NYPD_Shooting_Incident_Data__Historic_.csv")
char_vector <- c("OCCUR_DATE","BORO","LOCATION_DESC","PERP_AGE_GROUP","PERP_SEX","PERP_RACE","VIC_AGE_GROUP","VIC_SEX","VIC_RACE","Lon_Lat")
nypd_df[char_vector][is.na(nypd_df[char_vector])] <- "UNKNOWN"


```


#### Reading the NYPD data set and performed the cleaning of character columns by filling UNKNOWN for NA's


```{r}
death_data<- nypd_df %>% filter(nypd_df$STATISTICAL_MURDER_FLAG== TRUE)

tab1(death_data$VIC_RACE, cex.main=1, cex.name=0.8, cex.axis=0.8, cex.lab=0.8, sort.group ="decreasing", bar.values ="percent", main = paste(GetFigureCount()," | Percentage of victim's died"), xlab=" % of shoot outs", ylab="Race of Victims", horiz = T)

```


#### As the business question is to find the relationship between the race of people that lead to the victim's death, I filtered the data set based on the victim's death. As we can see, there are around 4,127 records of shootings that lead to the victim's deaths. In that, the highest number of victims who died were of Black Race with whopping 70%, followed by White Hispanics with 15.9% 



```{r}
death_data$PERP_RACE <- sub("^$", "UNKNOWN", death_data$PERP_RACE)#replacing empty cells with UNKNOWN
death_data$VIC_RACE <- sub("^$", "UNKNOWN", death_data$VIC_RACE)#replacing empty cells with UNKNOWN

crosstable2<- table(death_data$PERP_RACE,death_data$VIC_RACE)
crosstable2 <- round(prop.table(crosstable2)*100, 2)
crosstable2<-kable(crosstable2, caption = paste(GetFigureCount(), " | Percentage of Perpetrator's race killing victim's race"), xlab="test") %>% 
    kableExtra::kable_styling(., position = "float_left")
crosstable2

```



#### Intending to find the relationship between Perpetrator and victim's race, we created the contingency table.

#### We first convert the data as a table.


```{r fig.height = 7, fig.width = 18.5}
#library("gplots")
# 1. convert the data as a table
test<- table(death_data$PERP_RACE,
             death_data$VIC_RACE)
dt <- as.table(as.matrix(test))
```

#### Now, we use the converted data to visualize the Perpetrator's race killing the victim's race.

```{r fig.height = 7, fig.width = 18.5}
# 2. Graph
balloonplot(t(dt), 
            main =paste(GetFigureCount(), " | Perpetrator's race killing victim's race"), 
            xlab ="Victim's death", 
            ylab="Perpetrator's Race",
            label = FALSE, 
            show.margins = FALSE, 
            cex.main = 0.9  )
```


#### From the above graph, we can observe that Black and Unknown or white-Hispanic races murdered most Blacks. Unfortunately, the following significant pattern can be found where White-Hispanic were majorly murdered by White Hispanic themselves and next highest by BLACK people.



#### Even though contingency tables show some relationship between, Perpetrator and the victim's race, let us identify by Performing a Chi-square test at 0.05 significant level to identify the relationship between two categorical variables in the contingency table.


### `df=(r−1)(c−1)` degrees of freedom and `p = 0.05` *("Chi-Square Test of Independence in R - Easy Guides - Wiki - STHDA", 2020).*

#### Here,
#### r is the number of rows in the contingency table
#### c is the number of column in the contingency table


```{=html}

<B><FONT SIZE =3 color="#778899">
Step a: State the hypothesis and identify the claim</font></B>
<br> 
<B>Null hypothesis</B> 
$\it{H_{0}}$: There is no relationship exists between the race of perpetrator and victim.
<br>
<B>Alternative hypothesis</B> 
$\it{H_{1}}$: There is relationship between the race of perpetrator and victim.
 
<B><FONT SIZE =3 color="#778899">
Step b: Find the critical value</FONT>
</B>

```


```{r}
#here rows=6 columns=6 so df= 5*5

Critical_Val = qchisq(.05, df=25)

cat("The Critical value is ",Critical_Val)


```

```{=html}

<B><FONT SIZE =3 color="#778899">Step c: Performing $\chi^2$ Test or find the p-value</FONT></B>


```


```{r}
output_task4 <-chisq.test(dt)
output_task4
cat("Our Decision mustbe :", ifelse(output_task4$p.value < 0.05, "\nReject the null Hypothesis", "\nFail to reject the null hypothesis" ))

```

```{=html}

<B><FONT SIZE =3 color="#778899">Step d: Make the decision</FONT></B>
<BR>The decision rule is that if $p$-value is lesser than $\alpha$ we must reject the null hypothesis.
$p$-value < $\alpha$ i.e. 0.00000000000000022 is lesser than  0.05 ,so we must reject the $\it{H_{0}}$

<B><FONT SIZE =3 color="#778899">Step e: Summary</FONT></B>
<BR>
```

#### Interpretation: The evidence leads us to reject the null hypothesis. Therefore, there is a relationship between the race of Perpetrator and victim.

#### Another approach to figure out the relationship between categorical variables in R is by using Cramer's V coefficient



```{r fig.height = 15, fig.width = 18.5}
##library(vcd)

assocstats(table(death_data$PERP_RACE, death_data$VIC_RACE))

```
### interpretation of this function:
##### 1. Cramer’s V varies from 0 to 1,  a value of 1 indicates perfect association.In this scenario Cramer's V co-efficient is 0.334 which shows a moderate effect due to association.A value between `0.30 – < 0.50` is considered to be moderate. 


#### 2. Contingency coefficient values also vary between 0 to 1. The larger the contingency coefficient, the stronger the association. 0.5 again shows the moderate association between Perpetrator and victim's race. 


#### The discipline, the relevant data, and the analyst's goals all influence how measures of association are interpreted. There are sometimes guidelines for “small,” “medium,” and “large” impact. A smaller effect size may be considered “large,” but in physical science, such as chemistry, it may be considered very small in psychology or behavioral science. The unique circumstances of the study are necessary as well.


```{=html}
<br><hr><br>
```


## 2.	Is there any noticeable relationship between the victim's Sex to the Perpetrator's Race and age with a motive to recognize the Perpetrator's race pattern? *Major contribution by Neil Mascarenhas*

#### The NYPD is facing difficulties in identifying the reason behind the shooting. They are interested in seeing the relation between the Perpetrator's Race with Victim's sex and Age Group.


#### Here, we will use several modeling techniques to determine the pattern. First, we need to know the relationship between the Perpetrator's Race to the victim's Sex. We will create the model for these variables and understand what the variable that determines the relationship is. Moreover, which are the variables that have the highest relationship. Knowing this will help us predict and recognize the Perpetrator's Race. 


```{r}
BQ2 <- select(NYPD, c(PERP_RACE, VIC_SEX, VIC_AGE_GROUP)) %>% na.omit() %>% filter(PERP_RACE != "UNKNOWN", VIC_SEX != "U", VIC_AGE_GROUP != "UNKNOWN", PERP_RACE != "AMERICAN INDIAN/ALASKAN NATIVE")

set.seed(11521)
BQ2 <- BQ2[sample(nrow(BQ2)), ] ### Shuffles


```


```{r}

All_males<-BQ2 %>% filter(VIC_SEX=="M" ) %>% head(table(BQ2$VIC_SEX)[1])
All_females<-BQ2 %>% filter(VIC_SEX=="F") %>% head(table(BQ2$VIC_SEX)[1])

males_females <-  rbind(All_males, All_females)

set.seed(11521)
BQ2 <- males_females[sample(nrow(males_females)), ] ### Shuffles


#describe(BQ2)

cat("Any missing or NA values? \n(0 = No)\nResult = ",sum(!complete.cases(BQ2)))


##

```


#### Even though the tables show some sort of relationship between the variables, let us identify by Performing the Chi-square test of Independence at 0.05 significant level to identify the relationship between three categorical variables in the table above.

```{r warning=FALSE}
alpha = 0.05

LoSig   = 1-alpha
DegFre  = (nrow(BQ2) -1)*(ncol(BQ2) -1 )


```


### Step 1: State the hypotheses and identify the claims

## Claim 1

* Null hypothesis $H_0:$ The Victim Sex is independent of Perpetrator Race

* Alternative hypothesis $H_1:$ The Victim Sex is dependent upon Perpetrator Race

## Claim 2

* Null hypothesis $H_0:$ The Victim Age Group is independent of Perpetrator Race

* Alternative hypothesis $H_1:$ The Victim Age Group is dependent upon Perpetrator Race


### Step 2: Find the critical value.

```{r}

Critical_Val <-qchisq(p=LoSig, DegFre)

cat("The Critical value for both claims is ", Critical_Val)

```


### Step 3: Compute the test value.



```{r warning=FALSE}


Table_PR_VS <- table(BQ2$VIC_SEX,BQ2$PERP_RACE) # Vic Sex and Prep Race are stored in TAB
Table_PR_VA <- table(BQ2$PERP_RACE,BQ2$VIC_AGE_GROUP) # Vic Age Group and Vic Sex are stored in TAB


ChT_PR_VS <- chisq.test(Table_PR_VS, correct = T)
ChT_PR_VA <-chisq.test(Table_PR_VA, correct = T)

# ChT_PR_VS$p.value
# ChT_PR_VG$p.value


```

### Step 4: Make the decision.

```{r}


cat(("Determine Dependancy between and Victim Sex "),
ifelse(ChT_PR_VS$statistic < Critical_Val, 
      "Exists", 
       "Does Not Exist" ), " and the Test Score is ", ChT_PR_VS$statistic )

cat("\n \n \n")


cat(("Determine Dependancy between Perpetrator Race and Victim Age Group "),
ifelse(ChT_PR_VA$statistic < Critical_Val, 
       "Exists", 
       "Does Not Exist" ), " and the Test Score is ", ChT_PR_VA$statistic)




```



#### From the above tests, we computed the Chi-Square statistic and the critical value. We see that the Chi-Square statistic is **Greater** than the Critical value. Therefore we have enough evidence to **Reject** the null hypothesis H0 for **0.05** degree of freedom.

#### Now that we have made the decision, let us cross-verify using the p-value


```{r}


cat(("Determine Dependancy between and Victim Sex "),
ifelse(ChT_PR_VS$p.value < alpha, 
      "Exists", 
       "Does Not Exist" ), " and the P Value is ", ChT_PR_VS$statistic )

cat("\n \n \n")


cat(("Determine Dependancy between Perpetrator Race and Victim Age Group "),
ifelse(ChT_PR_VA$p.value < alpha, 
       "Exists", 
       "Does Not Exist" ), " and the P Value is ", ChT_PR_VA$p.value)



```

#### We can see that the p-value is significantly less than the decided alpha. Therefore we have made the right decision to reject the null hypothesis.


### Step 5: Summarize the results.

#### As we have enough evidence to reject the null hypothesis H0 for both the claims, Which states The Victim Sex is dependent upon Perpetrator Race and Victim Age Group



#### Now, we proceed with preparing the data for modeling. Here we split the data between the test and train dataset.

```{r}

set.seed(11521)
trainIndex <- createDataPartition(BQ2$VIC_SEX, p = 0.7, list = FALSE, times = 1)



BQ2_train <- BQ2[ trainIndex,]
BQ2_test <- BQ2[-trainIndex,]


table(NYPD$VIC_SEX)  %>% t()%>%
  kbl(caption = paste(GetFigureCount()," | Male and Female count in Entire dataset"),
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)


table(BQ2_train$VIC_SEX)  %>% t()  %>%
  kbl(caption = "Male and Female count in Train dataset",
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)

table(BQ2_test$VIC_SEX)%>% t()  %>%
  kbl(caption = "Male and Female count in Test dataset",
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)

```




#### We have successfully split the dataset into two different and unique sets. We have 70% of the dataset for train and 30% of the dataset for the test. Let us have a glimpse of those two sets.

```{r}


headTail(BQ2_train,top = 5, bottom = 0) %>%
  kbl(caption = paste(GetFigureCount()," | NYPD Train set : 70%"),
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)


headTail(BQ2_test,top = 5, bottom = 0)%>%
  kbl(caption = paste(GetFigureCount()," |NYPD Test set : 30%"),
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)



```

<br><br>

<hr style="height:3px;border-width:0;background-color:#cf2e44;margin: 0%; padding: 0%;border: 0px;">

<br><br>


# **Fitting the Models.**

#### As mentioned earlier, we will fit several models. Below are the models our team came up with and agreed upon.

### 1) Support Vector Machine 
### 2) Decision Trees 
### 3) Logistic regression 
### 4) Generalized Linear Model 
### 5) Random Forest 
### 6) Generalized Boosted Regression Modelling

#### Let us now fit them and compare their performance later.

```{r warning=FALSE, perform-AllMods, cache=TRUE}
###  VIC_SEX ~ VIC_AGE_GROUP * PERP_RACE 


# prepare training scheme
control <- trainControl(method="repeatedcv", number=5, repeats=2, sampling = "up", allowParallel=TRUE)

# train the SVM model
set.seed(15)
modelSVM <- train(VIC_SEX~., data=BQ2_train, method="svmRadial", trControl=control)

# train the GLM model
set.seed(15)
modelGLM <- train(VIC_SEX~., data=BQ2_train, method="glm", trControl=control,  family = "binomial",na.action = "na.exclude")

# train the RF model
set.seed(15)
modelRF <- train(VIC_SEX~., data=BQ2_train, method="rf", trControl=control)

# train the Log Reg model
set.seed(15)
modelLOGR <- train(VIC_SEX~., data=BQ2_train, method="multinom", trControl=control, verbose=FALSE, trace=FALSE)

# train the gbm model
set.seed(15)
modelGBM <- train(VIC_SEX~., data=BQ2_train, method="gbm", trControl=control, verbose=FALSE)

# train the Decison trees model
set.seed(15)
modelDTREE <- train(VIC_SEX~., data=BQ2_train, method="rpart", trControl=control)
```

#### We have generated several models. Let us now cumulate the results and compare the efficiency.


```{r Disp-AllMods, warning=FALSE, cache=TRUE}
# collect resamples
results <- resamples(list(SVM=modelSVM, 
                          RF=modelRF, 
                          GLM=modelGLM,
                          MULTM=modelLOGR, 
                          GBM=modelGBM, 
                          DTREE=modelDTREE))

summary(results)
```

#### The above results show us the summary of the model's performance. It has the Accuracy and Kappa metrics. We will use both to determine which one did best.

#### We can see that Random forest `RF`, Generalised Linear model `GLM` and logistic regression model `MULTM` did best with the accuracy of **63.2%** by seeing the maximum accuracy. 

#### It is difficult to compare as the accuracy is the same for all three. 

#### Therefore, we try to visualize it, hoping it might help to compare the results.

```{r warning=FALSE}
# boxplots of results
bwplot(results, main=paste(GetFigureCount()," | Kappa and Accuracy Matrics"))


```


#### From the above figure, we can see that the kappa max value was highest among all the other models for Random forest. We also see that for GLM and Logistic Regression model, the max score was not in the third quartile. This shows that they performed well a few times compared to random forest.

### We choose to go ahead with Random Forest as our desired model for further analysis.


#### Before we fit the model, we needed to find out which parameters were best and use them in our final models

#### We decided to go with cross-validation and perform the grid algorithm to determine the features.



```{r perform-CV-Grid, cache=TRUE}

# Define the control
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

set.seed(1234)
# Run the model
rf_default <- train(VIC_SEX~.,
    data = BQ2_train,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl)
# Print the results
print(rf_default)

```


#### After doing Grid and Cross-validation, we found out that our model accuracy got decreased. It was **63.3%** and now looking at the above result we see it it is at **58.8%**


#### To figure out why this shift was caused. We then decided to plot the results and visually see the error rate.


```{r perform-remodel500, warning=FALSE, cache=TRUE}

num.var     <- ncol(BQ2_train)

BQ2_train_rf = BQ2_train %>% mutate_if(is.character, as.factor)
#BQ2_train_rf$VIC_SEX = as.numeric(BQ2_train$VIC_SEX)

RF_Mod<-randomForest::randomForest(VIC_SEX~.,data = BQ2_train_rf,
                     mtry = num.var - 1, # try all variables at each split, except the response variable
                     proximity = TRUE,
                     importance = TRUE,
                     ntree = 500)


plot(RF_Mod, main=paste(GetFigureCount()," | Random Forest Trees and error"),lwd=2)
legend("bottomright",
c("Sex"),
fill=c("black")
)
```


#### As we can see above for Victim Race, the error was very high. However, it was not high as in the initial iterations. However, as the iterations increased, the rate increased and remained constant throughout the Age group. The error rate had spiked, and then it reduced. It was then constant throughout. Lastly, we see for Victim Sex that the error was moderate and kept constant throughout. Entirely we can see that the error rate fluctuated initially but later kept constant throughout the entire 500 trees.

# **Now, we create a confusion matrix and report the results of your model predictions on the train set. We will interpret and discuss the confusion matrix.**

#### A confusion matrix is a handy tool for calibrating the output of a model and examining all possible outcomes of our predictions. We see the values such as true positive, true negative, false positive, false negative.

#### We First need to have a prediction matrix to get the confusion matrix. Let us generate and use it.

```{r}
# c("link", "response", "terms", vector, prob, class, raw )
#
predicted <- predict(rf_default, BQ2_train)  # predicted scores



```



##### We have created the prediction matrix for the training dataset. Now we will use these results to get the confusion matrix. Which will show us how well did the model predict the values. This is to see for values that are known. Later, we will use a test dataset for values unknown to the model and see how well it does on unseen data.



```{r}
table(Pred = predicted, True =BQ2_train$VIC_SEX)%>%
  kbl(caption = paste(GetFigureCount()," | Confussion matrix for Train"),
      align = 'c') %>%
        kable_classic_2(full_width = T,
                        position = "center",
                        fixed_thead = T)
```





```{r warning=FALSE}

BQ2_train$VIC_SEX = factor(BQ2_train$VIC_SEX)

levels(BQ2_train$VIC_SEX) = c("F", "M")


caret::confusionMatrix(predicted, BQ2_train$VIC_SEX) 


```
#### We generated the confusion matrics. As we can see above, To understand the confusion matrix and interpret it, we need to base it on assumptions. 

#### We see that our prediction model did not work very well. We see that there is a false positive present. 


#### From our analysis, we can say that **False Positives** are higher and are more damaging for the analysis. We feel this is not good. The type of error depends on which use case we need to have. What are we trying to predict, and what values we expect to get.


# **Reporting and interpreting metrics for Accuracy, Recall-Precision, and Specificity.**

```{r}

pred <- prediction(as.numeric(as.character(factor(predicted, labels = c(0,1)))), as.numeric(as.character(factor(BQ2_train$VIC_SEX, labels = c(0,1)))))

```

#### We have clreated the prediction matrics let us use it to dettermine the performances.


```{r}
## ROC curve
ROC.perf_tf <- performance(pred, "tpr", "fpr")


## Recall-Precision curve             
RP.perf_pr <- performance(pred, "prec", "rec")


## Cutoff 
RP.F_measure <- performance(pred, "f", x.measure="cutoff")

```


```{r}

#plot the curves
par(mfrow=c(1,3))
plot (RP.F_measure,main= paste(GetFigureCount()," Cut off"))
plot (RP.perf_pr, main=paste(GetFigureCount(),"\nPrecission and Recall"))
plot (ROC.perf_tf, main=paste(GetFigureCount(),"\nTrue and False Rate (Possitive)"))

## 

```

```{r warning=FALSE}

# Specificity and Sensitivity

c_mat  = caret::confusionMatrix(predicted, BQ2_train$VIC_SEX) 

# c_mat$table
# c_mat$table[1]
# c_mat$table[2]
# c_mat$table[3]
# c_mat$table[4]


sensitivity_Train <- c_mat$table[1]/(c_mat$table[1]+c_mat$table[2])

specificity_Train <- c_mat$table[4]/(c_mat$table[4]+c_mat$table[3])

cat("The sensitivity is ", sensitivity_Train, " and Specificity is ", specificity_Train)

```

#### Sensitivity is the percentage of `1`'s (actuals) correctly predicted by the model. It is also called True Positive Rate. Here our Sensitivity of the model is  **0.376** 

*Sensitivity= # Actual 1′s and Predicted as 1′s / # of Actual 1′s*


#### Specificity is the percentage of `0`'s (actuals) correctly predicted by the model. It is also called True Negative Rate. Here our Specificity of the model is  **0.82**. Specificity can also be calculated as 1 − False Positive Rate.


*Specificity=# Actual 0′s and Predicted as 0′s / # of Actual 0′s*

<br><br>

<hr style="height:3px;border-width:0;background-color:#cf2e44;margin: 0%; padding: 0%;border: 0px;">

<br><br>



# **Creating a confusion matrix and reporting the results of our model for the test set.**

```{r warning=FALSE}

# c("link", "response", "terms", vector, prob, class, raw )
#
predicted_Test <- predict(rf_default, BQ2_test)  # predicted scores


BQ2_test$VIC_SEX = factor(BQ2_test$VIC_SEX)
levels(BQ2_test$VIC_SEX) = c("F",'M')

caret::confusionMatrix(BQ2_test$VIC_SEX, predicted_Test)


```

#### Here, we again generated confusion matrics. As we can see above, it is the default threshold.

#### We see that our prediction model did not work well. We see that there is a false positive present.


<br><br>

<hr style="height:3px;border-width:0;background-color:#cf2e44;margin: 0%; padding: 0%;border: 0px;">

<br><br>




# **Calculating and interpret the AUC**


```{r}

## Accuracy

auc.tmp <- performance(pred,"auc");
auc <- as.numeric(auc.tmp@y.values)
cat("ROC area under the curve for train is = ", auc, "\n")


predicted_Test <- predict(rf_default, BQ2_test)  # predicted scores

pred <- prediction(as.numeric(as.character(factor(predicted_Test, labels = c(0,1)))), as.numeric(as.character(factor(BQ2_test$VIC_SEX, labels = c(0,1)))))


auc.tmp <- performance(pred,"auc");
auc <- as.numeric(auc.tmp@y.values)
cat("ROC area under the curve for test is = ", auc, "\n")


```

##### Let us dig deep and understand what the value means. We see above that the ROC area under the curve is =  **0.598**. This value is a measurement of the efficiency of the model. Receiver Operating Characteristic (ROC) curves are a popular way to visualize the tradeoffs between Sensitivity and Specificity in a binary classifier. The probabilistic interpretation is that if we randomly choose an optimistic case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC. The matrix cells enumerate all possible combinations of positive and negative cases, and the fraction under the curve comprises the cells where the positive case outranks the negative one.



```{=html}
<br><hr><br>
```
